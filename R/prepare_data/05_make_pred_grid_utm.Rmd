---
title: "Make the prediction grid"
author: "Max Lindmark"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
  df_print: paged
pdf_document: default
editor_options: 
  chunk_output_type: console
---
  
```{r setup, include = FALSE, cache=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 12,
  fig.asp = 0.618,
  fig.align ='center'
)
```

# Intro
Make an evenly spaced UTM prediction grid with all spatially varying covariates.

```{r lib, message=FALSE}
# Load libraries, install if needed
library(tidyverse)
library(tidylog)
library(viridis)
library(mapdata)
library(rgdal)
library(raster)
library(sf)
library(sp)
library(marmap)
library(rnaturalearth)
library(rnaturalearthdata)
library(patchwork)
library(ncdf4)
library(chron)
library(gganimate)
library(gifski)
library(png)
library(RCurl)
library(sdmTMB)
library(RColorBrewer)
library(terra)
library(mapplots)
library(readxl)

# Source UTM function
source("/Users/maxlindmark/Dropbox/Max work/R/cod_interactions/R/functions/lon_lat_utm.R")

# Source code for map plots
source("/Users/maxlindmark/Dropbox/Max work/R/cod_interactions/R/functions/map_plot.R")

theme_set(theme_plot())
```

## Basic grid with depth

```{r basic grid with depth, message = FALSE}
# These ranges are based on the diet data. Convert these to UTM!
ymin = 54
ymax = 59
xmin = 12
xmax = 22

# These are the years we are using
diet_years <- c(1993:2020)

LongLatToUTM(x = 11, y = 53, zone = 33)
LongLatToUTM(11, 58, 33)
LongLatToUTM(22, 53, 33)
LongLatToUTM(22, 58, 33)

# Round values based on above
utm_x_min <- 230000
utm_x_max <- 960000
utm_y_min <- 5900000
utm_y_max <- 6560000

# Make the evenly spaced (on UTM) grid 
pred_grid <- expand.grid(
  X = seq(utm_x_min, utm_x_max, by = 4000),
  Y = seq(utm_y_min, utm_y_max, by = 4000)) # 4x4 km

plot_map + 
  geom_point(data = pred_grid, aes(x = X, y = Y), alpha = 0.1, shape = 21, fill = NA) +
  labs(x = "Longitude", y = "Latitude")

# Looks OK!
```

## Add environmental covariates

### Depth

```{r}
west <- raster("data/depth_geo_tif/D5_2018_rgb-1.tif")
plot(west)

east <- raster("data/depth_geo_tif/D6_2018_rgb-1.tif")
plot(east)

dep_rast <- raster::merge(west, east)

plot(dep_rast)

# Do that by extracting depths of the pred grid
utm_coords <- pred_grid %>% dplyr::select(X, Y)

# Reproject the raster to fit the UTM pred grid...
# Define spatial reference 
sr <- "+proj=utm +zone=33  +datum=WGS84 +units=m " 

# Project Raster... This takes some time
projected_raster <- projectRaster(dep_rast, crs = sr)

utm_coords$depth <- extract(projected_raster, utm_coords[, 1:2])
max(utm_coords$depth)
min(utm_coords$depth)

hist(utm_coords$depth)

# Convert to depth (instead of elevation)
ggplot(utm_coords, aes(depth)) + geom_histogram()
utm_coords$depth <- utm_coords$depth - max(utm_coords$depth)
ggplot(utm_coords, aes(depth)) + geom_histogram()

plot_map + 
  geom_point(data = filter(utm_coords, depth < 0), aes(x = X, y = Y, color = depth)) +
  scale_colour_viridis()

df <- utm_coords %>% filter(depth < 0) %>% mutate(depth = depth*-1)

# Now make a new grid
pred_grid <- data.frame(X = rep(df$X, length(unique(diet_years))),
                        Y = rep(df$Y, length(unique(diet_years))),
                        depth = rep(df$depth, length(unique(diet_years))),
                        year = rep(sort(unique(diet_years)), each = nrow(df)))


pred_grid <- pred_grid %>% mutate(deep = ifelse(depth > 135, "Y", "N"))

plot_map + 
  geom_raster(data = filter(pred_grid, year == "1999"), aes(x = X, y = Y, fill = deep))

plot_map + 
  geom_raster(data = filter(pred_grid, year == "1999"), aes(x = X, y = Y, fill = depth))

hist(pred_grid$depth)

pred_grid <- pred_grid %>% dplyr::select(-deep)
```

### Substrate
First add lat and lon based on X and Y (utm)

```{r lat long, message=FALSE}
# Need to go from UTM to lat long for this one... 
# https://stackoverflow.com/questions/30018098/how-to-convert-utm-coordinates-to-lat-and-long-in-r
xy <- as.matrix(pred_grid[, 1:2])
v <- vect(xy, crs="+proj=utm +zone=33 +datum=WGS84  +units=m")
y <- project(v, "+proj=longlat +datum=WGS84")
lonlat <- geom(y)[, c("x", "y")]

pred_grid$lon <- lonlat[, 1]
pred_grid$lat <- lonlat[, 2]
```

```{r}
substrate <- raster("data/substrate_tif/BALANCE_SEABED_SEDIMENT.tif")
substrate_longlat = projectRaster(substrate, crs = ('+proj=longlat'))

# Now extract the values from the saduria raster to pred_grid
pred_grid$substrate <- extract(substrate_longlat, pred_grid %>% dplyr::select(lon, lat))

unique(pred_grid$substrate)

# Plot
ggplot(pred_grid, aes(lon, lat, color = substrate)) + 
  geom_point()

factor(sort(unique(round(pred_grid$substrate))))

pred_grid$substrate <- round(pred_grid$substrate)

pred_grid <- pred_grid %>% mutate(substrate = ifelse(substrate == 1, "bedrock", substrate),
                                  substrate = ifelse(substrate == 2, "hard-bottom complex", substrate),
                                  substrate = ifelse(substrate == 3, "sand", substrate),
                                  substrate = ifelse(substrate == 4, "hard clay", substrate),
                                  substrate = ifelse(substrate == 5, "mud", substrate))
# I. Bedrock.
# II. Hard bottom complex, includes patchy hard surfaces and coarse sand (sometimes also clay) to boulders.
# III. Sand including fine to coarse sand (with gravel exposures).
# IV. Hard clay sometimes/often/possibly exposed or covered with a thin layer of
# sand/gravel.
# V. Mud including gyttja-clay to gyttja-silt.

# Plot
ggplot(pred_grid, aes(lon, lat, color = substrate)) + 
  geom_point()
```

### Oxygen

```{r oxygen}
# Add quarter
pred_grid_1 <- pred_grid %>% mutate(quarter = 1)
pred_grid_4 <- pred_grid %>% mutate(quarter = 4)

dat <- bind_rows(pred_grid_1, pred_grid_4)

# Downloaded from here: https://resources.marine.copernicus.eu/?option=com_csw&view=details&product_id=BALTICSEA_REANALYSIS_BIO_003_012
# Extract raster points: https://gisday.wordpress.com/2014/03/24/extract-raster-values-from-points-using-r/comment-page-1/
# https://rpubs.com/boyerag/297592
# https://pjbartlein.github.io/REarthSysSci/netCDF.html#get-a-variable
# Open the netCDF file
ncin <- nc_open("data/NEMO_Nordic_SCOBI/dataset-reanalysis-scobi-monthlymeans_1664182224542.nc")

print(ncin)

# Get longitude and latitude
lon <- ncvar_get(ncin,"longitude")
nlon <- dim(lon)
head(lon)

lat <- ncvar_get(ncin,"latitude")
nlat <- dim(lat)
head(lat)

# Get time
time <- ncvar_get(ncin,"time")
time

tunits <- ncatt_get(ncin,"time","units")
nt <- dim(time)
nt
tunits

# Get oxygen
dname <- "o2b"

oxy_array <- ncvar_get(ncin,dname)
dlname <- ncatt_get(ncin,dname,"long_name")
dunits <- ncatt_get(ncin,dname,"units")
fillvalue <- ncatt_get(ncin,dname,"_FillValue")
dim(oxy_array)

# Get global attributes
title <- ncatt_get(ncin,0,"title")
institution <- ncatt_get(ncin,0,"institution")
datasource <- ncatt_get(ncin,0,"source")
references <- ncatt_get(ncin,0,"references")
history <- ncatt_get(ncin,0,"history")
Conventions <- ncatt_get(ncin,0,"Conventions")

# Convert time: split the time units string into fields
tustr <- strsplit(tunits$value, " ")
tdstr <- strsplit(unlist(tustr)[3], "-")
tmonth <- as.integer(unlist(tdstr)[2])
tday <- as.integer(unlist(tdstr)[3])
tyear <- as.integer(unlist(tdstr)[1])

# Here I deviate from the guide a little bit. Save this info:
dates <- chron(time, origin = c(tmonth, tday, tyear))

# Crop the date variable
months <- as.numeric(substr(dates, 2, 3))
years <- as.numeric(substr(dates, 8, 9))
years <- ifelse(years > 90, 1900 + years, 2000 + years)

# Replace netCDF fill values with NA's
oxy_array[oxy_array == fillvalue$value] <- NA

# Next, we need to work with the months that correspond to the quarters that we use.
# loop through each time step, and if it is a good month save it as a raster.
# First get the index of months that correspond to Q4
months

index_keep_q1 <- which(months < 4)
index_keep_q4 <- which(months > 9)

oxy_q1 <- oxy_array[, , index_keep_q1]
oxy_q4 <- oxy_array[, , index_keep_q4]

months_keep_q1 <- months[index_keep_q1]
months_keep_q4 <- months[index_keep_q4]

years_keep_q1 <- years[index_keep_q1]
years_keep_q4 <- years[index_keep_q4]

# Now we have an array with data for that quarter
# We need to now calculate the average within a year.
# Get a sequence that takes every third value between 1: number of months (length)
loop_seq_q1 <- seq(1, dim(oxy_q1)[3], by = 3)
loop_seq_q4 <- seq(1, dim(oxy_q4)[3], by = 3)

# Create objects that will hold data
dlist_q1 <- list()
dlist_q4 <- list()

oxy_1 <- c()
oxy_2 <- c()
oxy_3 <- c()
oxy_ave_q1 <- c()

oxy_10 <- c()
oxy_11 <- c()
oxy_12 <- c()
oxy_ave_q4 <- c()

# Now average by quarter. The vector loop_seq_q1 is 1, 4, 7 etc. So first i is 1, 2, 3,
# which is the index we want. 

for(i in loop_seq_q1) { # We can use q1 as looping index, doesn't matter!
  
  oxy_1 <- oxy_q1[, , (i)]
  oxy_2 <- oxy_q1[, , (i + 1)]
  oxy_3 <- oxy_q1[, , (i + 2)]
  
  oxy_10 <- oxy_q4[, , (i)]
  oxy_11 <- oxy_q4[, , (i + 1)]
  oxy_12 <- oxy_q4[, , (i + 2)]
  
  oxy_ave_q1 <- (oxy_1 + oxy_2 + oxy_3) / 3
  oxy_ave_q4 <- (oxy_10 + oxy_11 + oxy_12) / 3
    
  list_pos_q1 <- ((i/3) - (1/3)) + 1 # to get index 1:n(years)
  list_pos_q4 <- ((i/3) - (1/3)) + 1 # to get index 1:n(years)
  
  dlist_q1[[list_pos_q1]] <- oxy_ave_q1
  dlist_q4[[list_pos_q4]] <- oxy_ave_q4

}

# Now name the lists with the year:
names(dlist_q1) <- unique(years_keep_q1)
names(dlist_q4) <- unique(years_keep_q4)

# Now I need to make a loop where I extract the raster value for each year...

# Filter years in the pred_grid based on quarter
d_sub_oxy_q1 <- dat %>% filter(quarter == 1)
d_sub_oxy_q4 <- dat %>% filter(quarter == 4)

# Create data holding object
oxy_data_list_q1 <- list()
oxy_data_list_q4 <- list()

# Create factor year for indexing the list in the loop
d_sub_oxy_q1$year_f <- as.factor(d_sub_oxy_q1$year)
d_sub_oxy_q4$year_f <- as.factor(d_sub_oxy_q4$year)

# Loop through each year and extract raster values for the data points
for(i in sort(unique(d_sub_oxy_q1$year_f))) { # We can use q1 as looping index, doesn't matter!
  
  # Set plot limits
  ymin = 54; ymax = 59; xmin = 12; xmax = 22

  # Subset a year
  oxy_slice_q1 <- dlist_q1[[i]]
  oxy_slice_q4 <- dlist_q4[[i]]
  
  # Create raster for that year (i)
  r_q1 <- raster(t(oxy_slice_q1), xmn = min(lon), xmx = max(lon), ymn = min(lat), ymx = max(lat),
                 crs = CRS("+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs+ towgs84=0,0,0"))
  r_q4 <- raster(t(oxy_slice_q4), xmn = min(lon), xmx = max(lon), ymn = min(lat), ymx = max(lat),
                 crs = CRS("+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs+ towgs84=0,0,0"))
  
  # Flip...
  r_q1 <- flip(r_q1, direction = 'y')
  r_q4 <- flip(r_q4, direction = 'y')
  
  plot(r_q1, main = paste(i, "Q1"))
  plot(r_q4, main = paste(i, "Q4"))
  
  # Change projection to UTM (same as pred grid)
  proj_raster_q1 <- projectRaster(r_q1, crs = sr)
  proj_raster_q4 <- projectRaster(r_q4, crs = sr)
  
  # Filter the same years and select only coordinates
  d_slice_q1 <- d_sub_oxy_q1 %>% filter(year_f == i) %>% dplyr::select(X, Y)
  d_slice_q4 <- d_sub_oxy_q4 %>% filter(year_f == i) %>% dplyr::select(X, Y)
  
  # Make into a SpatialPoints object
  data_sp_q1 <- SpatialPoints(d_slice_q1, proj4string = CRS(sr))
  data_sp_q4 <- SpatialPoints(d_slice_q4, proj4string = CRS(sr))
  
  # Extract raster value (oxygen)
  rasValue_q1 <- raster::extract(proj_raster_q1, data_sp_q1)
  rasValue_q4 <- raster::extract(proj_raster_q4, data_sp_q4)
  
  # Add in the raster value in the df holding the coordinates for the data
  d_slice_q1$oxy <- rasValue_q1
  d_slice_q4$oxy <- rasValue_q4
  
  # Add in which year
  d_slice_q1$year <- i
  d_slice_q4$year <- i

  # Now the unit of oxygen is mmol/m3. I want it to be ml/L. The original model is in unit ml/L
  # and it's been converted by the data host. Since it was converted without accounting for
  # pressure or temperature, I can simply use the following conversion factor:
  # 1 ml/l = 103/22.391 = 44.661 μmol/l -> 1 ml/l = 0.044661 mmol/l = 44.661 mmol/m^3 -> 0.0223909 ml/l = 1mmol/m^3
  # https://ocean.ices.dk/tools/unitconversion.aspx

  d_slice_q1$oxy <- d_slice_q1$oxy * 0.0223909
  d_slice_q4$oxy <- d_slice_q4$oxy * 0.0223909
    
  # Create a index for the data last where we store all years (because our loop index
  # i is not continuous, we can't use it directly)
  index_q1 <- as.numeric(as.character(d_slice_q1$year))[1] - 1992
  index_q4 <- as.numeric(as.character(d_slice_q4$year))[1] - 1992
  
  # Add each years' data in the list
  oxy_data_list_q1[[index_q1]] <- d_slice_q1
  oxy_data_list_q4[[index_q4]] <- d_slice_q4

}

# Now create a data frame from the list of all annual values
big_dat_oxy_q1 <- dplyr::bind_rows(oxy_data_list_q1)
big_dat_oxy_q4 <- dplyr::bind_rows(oxy_data_list_q4)
big_dat_oxy <- bind_rows(mutate(big_dat_oxy_q1, quarter = 1),
                         mutate(big_dat_oxy_q4, quarter = 4))

# Create an ID for matching the oxygen data with the pred_grid
dat$id_oxy <- paste(dat$year, dat$quarter, dat$X, dat$Y, sep = "_")
big_dat_oxy$id_oxy <- paste(big_dat_oxy$year, big_dat_oxy$quarter, big_dat_oxy$X, big_dat_oxy$Y, sep = "_")

# Which id's are not in the cpue data (dat)? (It's because I don't have those years, not about the location)
ids <- dat$id_oxy[!dat$id_oxy %in% c(big_dat_oxy$id_oxy)]

unique(ids)
length(unique(ids))
length(unique(dat$id_oxy))

# Select only the columns we want to merge
big_dat_sub_oxy <- big_dat_oxy %>% dplyr::select(id_oxy, oxy)

# Remove duplicate ID (one oxy value per id)
big_dat_sub_oxy %>% group_by(id_oxy) %>% mutate(n = n()) %>% arrange(desc(n))
big_dat_sub_oxy2 <- big_dat_sub_oxy %>% distinct(id_oxy, .keep_all = TRUE)
```

### Temperature

```{r temperature}
# Open the netCDF file
ncin <- nc_open("data/NEMO_Nordic_SCOBI/dataset-reanalysis-nemo-monthlymeans_1664183191233.nc")

print(ncin)

# Get longitude and latitude
lon <- ncvar_get(ncin,"longitude")
nlon <- dim(lon)
head(lon)

lat <- ncvar_get(ncin,"latitude")
nlat <- dim(lat)
head(lat)

# Get time
time <- ncvar_get(ncin,"time")
time

tunits <- ncatt_get(ncin,"time","units")
nt <- dim(time)
nt
tunits

# Get temperature
dname <- "bottomT"

temp_array <- ncvar_get(ncin,dname)
dlname <- ncatt_get(ncin,dname,"long_name")
dunits <- ncatt_get(ncin,dname,"units")
fillvalue <- ncatt_get(ncin,dname,"_FillValue")
dim(temp_array)

# Get global attributes
title <- ncatt_get(ncin,0,"title")
institution <- ncatt_get(ncin,0,"institution")
datasource <- ncatt_get(ncin,0,"source")
references <- ncatt_get(ncin,0,"references")
history <- ncatt_get(ncin,0,"history")
Conventions <- ncatt_get(ncin,0,"Conventions")

# Convert time: split the time units string into fields
tustr <- strsplit(tunits$value, " ")
tdstr <- strsplit(unlist(tustr)[3], "-")
tmonth <- as.integer(unlist(tdstr)[2])
tday <- as.integer(unlist(tdstr)[3])
tyear <- as.integer(unlist(tdstr)[1])

# Here I deviate from the guide a little bit. Save this info:
dates <- chron(time, origin = c(tmonth, tday, tyear))

# Crop the date variable
months <- as.numeric(substr(dates, 2, 3))
years <- as.numeric(substr(dates, 8, 9))
years <- ifelse(years > 90, 1900 + years, 2000 + years)

# Replace netCDF fill values with NA's
temp_array[temp_array == fillvalue$value] <- NA

# Next, we need to work with the months that correspond to the quarters that we use.
# loop through each time step, and if it is a good month save it as a raster.
# First get the index of months that correspond to Q4
months

index_keep_q1 <- which(months < 4)
index_keep_q4 <- which(months > 9)

temp_q1 <- temp_array[, , index_keep_q1]
temp_q4 <- temp_array[, , index_keep_q4]

months_keep_q1 <- months[index_keep_q1]
months_keep_q4 <- months[index_keep_q4]

years_keep_q1 <- years[index_keep_q1]
years_keep_q4 <- years[index_keep_q4]

# Now we have an array with data for that quarter
# We need to now calculate the average within a year.
# Get a sequence that takes every third value between 1: number of months (length)
loop_seq_q1 <- seq(1, dim(temp_q1)[3], by = 3)
loop_seq_q4 <- seq(1, dim(temp_q4)[3], by = 3)

# Create objects that will hold data
dlist_q1 <- list()
dlist_q4 <- list()

temp_1 <- c()
temp_2 <- c()
temp_3 <- c()
temp_ave_q1 <- c()

temp_10 <- c()
temp_11 <- c()
temp_12 <- c()
temp_ave_q4 <- c()

# Now average by quarter. The vector loop_seq_q1 is 1, 4, 7 etc. So first i is 1, 2, 3,
# which is the index we want. 

for(i in loop_seq_q1) {
  
  temp_1 <- temp_q1[, , (i)]
  temp_2 <- temp_q1[, , (i + 1)]
  temp_3 <- temp_q1[, , (i + 2)]
  
  temp_10 <- temp_q4[, , (i)]
  temp_11 <- temp_q4[, , (i + 1)]
  temp_12 <- temp_q4[, , (i + 2)]
  
  temp_ave_q1 <- (temp_1 + temp_2 + temp_3) / 3
  temp_ave_q4 <- (temp_10 + temp_11 + temp_12) / 3
  
  list_pos_q1 <- ((i/3) - (1/3)) + 1 # to get index 1:n(years)
  list_pos_q4 <- ((i/3) - (1/3)) + 1 # to get index 1:n(years)
  
  dlist_q1[[list_pos_q1]] <- temp_ave_q1
  dlist_q4[[list_pos_q4]] <- temp_ave_q4
  
}

# Now name the lists with the year:
names(dlist_q1) <- unique(years_keep_q1)
names(dlist_q4) <- unique(years_keep_q4)

# Now I need to make a loop where I extract the raster value for each year...
# The data is called dat so far in this script

# Filter years in the data frame to only have the years I have temperature for
d_sub_temp_q1 <- dat %>% filter(quarter == 1) %>% filter(year %in% names(dlist_q1)) %>% droplevels()
d_sub_temp_q4 <- dat %>% filter(quarter == 4) %>% filter(year %in% names(dlist_q4)) %>% droplevels()

# Create data holding object
temp_data_list_q1 <- list()
temp_data_list_q4 <- list()

# Create factor year for indexing the list in the loop
d_sub_temp_q1$year_f <- as.factor(d_sub_temp_q1$year)
d_sub_temp_q4$year_f <- as.factor(d_sub_temp_q4$year)

# Loop through each year and extract raster values for the data points
for(i in unique(d_sub_temp_q1$year_f)) {
  
  # Set plot limits
  ymin = 54; ymax = 59; xmin = 12; xmax = 22
  
  # Subset a year
  temp_slice_q1 <- dlist_q1[[i]]
  temp_slice_q4 <- dlist_q4[[i]]
  
  # Create raster for that year (i)
  r_q1 <- raster(t(temp_slice_q1), xmn = min(lon), xmx = max(lon), ymn = min(lat), ymx = max(lat),
                 crs = CRS("+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs+ towgs84=0,0,0"))
  r_q4 <- raster(t(temp_slice_q4), xmn = min(lon), xmx = max(lon), ymn = min(lat), ymx = max(lat),
                 crs = CRS("+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs+ towgs84=0,0,0"))
  
  # Flip...
  r_q1 <- flip(r_q1, direction = 'y')
  r_q4 <- flip(r_q4, direction = 'y')
  
  plot(r_q1, main = paste(i, "Q1"))
  plot(r_q4, main = paste(i, "Q4"))
  
  # Change projection to UTM (same as pred grid)
  proj_raster_q1 <- projectRaster(r_q1, crs = sr)
  proj_raster_q4 <- projectRaster(r_q4, crs = sr)
  
  # Filter the same years and select only coordinates
  d_slice_q1 <- d_sub_temp_q1 %>% filter(year_f == i) %>% dplyr::select(X, Y)
  d_slice_q4 <- d_sub_temp_q4 %>% filter(year_f == i) %>% dplyr::select(X, Y)
  
  # Make into a SpatialPoints object
  data_sp_q1 <- SpatialPoints(d_slice_q1)
  data_sp_q4 <- SpatialPoints(d_slice_q4)
  
  # Extract raster value (temperature)
  rasValue_q1 <- raster::extract(proj_raster_q1, data_sp_q1)
  rasValue_q4 <- raster::extract(proj_raster_q4, data_sp_q4)
  
  # Now we want to plot the results of the raster extractions by plotting the
  # data points over a raster and saving it for each year.
  # Make the SpatialPoints object into a raster again (for pl)
  df_q1 <- as.data.frame(data_sp_q1)
  df_q4 <- as.data.frame(data_sp_q4)
  
  # Add in the raster value in the df holding the coordinates for the data
  d_slice_q1$temp <- rasValue_q1
  d_slice_q4$temp <- rasValue_q4
  
  # Add in which year
  d_slice_q1$year <- i
  d_slice_q4$year <- i
  
  # Create a index for the data last where we store all years (because our loop index
  # i is not continuous, we can't use it directly)
  index_q1 <- as.numeric(d_slice_q1$year)[1] - 1992
  index_q4 <- as.numeric(d_slice_q4$year)[1] - 1992
  
  # Add each years' data in the list
  temp_data_list_q1[[index_q1]] <- d_slice_q1
  temp_data_list_q4[[index_q4]] <- d_slice_q4
  
  }

# Now create a data frame from the list of all annual values
big_dat_temp_q1 <- dplyr::bind_rows(temp_data_list_q1)
big_dat_temp_q4 <- dplyr::bind_rows(temp_data_list_q4)
big_dat_temp <- bind_rows(mutate(big_dat_temp_q1, quarter = 1),
                          mutate(big_dat_temp_q4, quarter = 4))

# Create an ID for matching the temperature data with the data
dat$id_temp <- paste(dat$year, dat$quarter, dat$X, dat$Y, sep = "_")
big_dat_temp$id_temp <- paste(big_dat_temp$year, big_dat_temp$quarter, big_dat_temp$X, big_dat_temp$Y, sep = "_")

# Which id's are not in the data (dat)? (It's because I don't have those years, not about the location)
ids <- dat$id_temp[!dat$id_temp %in% c(big_dat_temp$id_temp)]
unique(ids)
length(unique(ids))
length(unique(dat$id_sal))

# Select only the columns we want to merge
big_dat_sub_temp <- big_dat_temp %>% dplyr::select(id_temp, temp)

# Remove duplicate ID (one temp value per id)
big_dat_sub_temp2 <- big_dat_sub_temp %>% distinct(id_temp, .keep_all = TRUE)
```

### Bottom salinity

```{r}
# https://data.marine.copernicus.eu/product/BALTICSEA_REANALYSIS_PHY_003_011/download?dataset=dataset-reanalysis-nemo-monthlymeans

# Open the netCDF file
ncin <- nc_open("data/NEMO_Nordic_SCOBI/dataset-reanalysis-nemo-monthlymeans_1668587452211.nc")

print(ncin)

# Get longitude and latitude
lon <- ncvar_get(ncin,"longitude")
nlon <- dim(lon)
head(lon)

lat <- ncvar_get(ncin,"latitude")
nlat <- dim(lat)
head(lat)

# Get time
time <- ncvar_get(ncin,"time")
time

tunits <- ncatt_get(ncin,"time","units")
nt <- dim(time)
nt
tunits

# Get Salinity
dname <- "sob"

sal_array <- ncvar_get(ncin,dname)
dlname <- ncatt_get(ncin,dname,"long_name")
dunits <- ncatt_get(ncin,dname,"units")
fillvalue <- ncatt_get(ncin,dname,"_FillValue")
dim(sal_array)

# Get global attributes
title <- ncatt_get(ncin,0,"title")
institution <- ncatt_get(ncin,0,"institution")
datasource <- ncatt_get(ncin,0,"source")
references <- ncatt_get(ncin,0,"references")
history <- ncatt_get(ncin,0,"history")
Conventions <- ncatt_get(ncin,0,"Conventions")

# Convert time: split the time units string into fields
tustr <- strsplit(tunits$value, " ")
tdstr <- strsplit(unlist(tustr)[3], "-")
tmonth <- as.integer(unlist(tdstr)[2])
tday <- as.integer(unlist(tdstr)[3])
tyear <- as.integer(unlist(tdstr)[1])

# Here I deviate from the guide a little bit. Save this info:
dates <- chron(time, origin = c(tmonth, tday, tyear))

# Crop the date variable
months <- as.numeric(substr(dates, 2, 3))
years <- as.numeric(substr(dates, 8, 9))
years <- ifelse(years > 90, 1900 + years, 2000 + years)

# Replace netCDF fill values with NA's
sal_array[sal_array == fillvalue$value] <- NA

# Next, we need to work with the months that correspond to the quarters that we use.
# loop through each time step, and if it is a good month save it as a raster.
# First get the index of months that correspond to Q4
months

index_keep_q1 <- which(months < 4)
index_keep_q4 <- which(months > 9)

sal_q1 <- sal_array[, , index_keep_q1]
sal_q4 <- sal_array[, , index_keep_q4]

months_keep_q1 <- months[index_keep_q1]
months_keep_q4 <- months[index_keep_q4]

years_keep_q1 <- years[index_keep_q1]
years_keep_q4 <- years[index_keep_q4]

# Now we have an array with data for that quarter
# We need to now calculate the average within a year.
# Get a sequence that takes every third value between 1: number of months (length)
loop_seq_q1 <- seq(1, dim(sal_q1)[3], by = 3)
loop_seq_q4 <- seq(1, dim(sal_q4)[3], by = 3)

# Create objects that will hold data
dlist_q1 <- list()
dlist_q4 <- list()

sal_1 <- c()
sal_2 <- c()
sal_3 <- c()
sal_ave_q1 <- c()

sal_10 <- c()
sal_11 <- c()
sal_12 <- c()
sal_ave_q4 <- c()

# Now average by quarter. The vector loop_seq_q1 is 1, 4, 7 etc. So first i is 1, 2, 3,
# which is the index we want. 

dim(sal_q1)
dim(sal_q4)

# Hmm, we didn't get the first month in the salinity series... repeat month 2 and fill in so the dimensions are correct
sal_q1 <- sal_q1[,,c(1, 1:83)]

dim(sal_q1)

for(i in loop_seq_q1) {
  
  sal_1 <- sal_q1[, , (i)]
  sal_2 <- sal_q1[, , (i + 1)]
  sal_3 <- sal_q1[, , (i + 2)]
  
  sal_10 <- sal_q4[, , (i)]
  sal_11 <- sal_q4[, , (i + 1)]
  sal_12 <- sal_q4[, , (i + 2)]
  
  sal_ave_q1 <- (sal_1 + sal_2 + sal_3) / 3
  sal_ave_q4 <- (sal_10 + sal_11 + sal_12) / 3
  
  list_pos_q1 <- ((i/3) - (1/3)) + 1 # to get index 1:n(years)
  list_pos_q4 <- ((i/3) - (1/3)) + 1 # to get index 1:n(years)
  
  dlist_q1[[list_pos_q1]] <- sal_ave_q1
  dlist_q4[[list_pos_q4]] <- sal_ave_q4
  
}

# Now name the lists with the year:
names(dlist_q1) <- unique(years_keep_q1)
names(dlist_q4) <- unique(years_keep_q4)

# Now I need to make a loop where I extract the raster value for each year...
# The cpue data is called dat so far in this script

# Filter years in the cpue data frame to only have the years I have salinity for
d_sub_sal_q1 <- dat %>% filter(quarter == 1) %>% filter(year %in% names(dlist_q1)) %>% droplevels()
d_sub_sal_q4 <- dat %>% filter(quarter == 4) %>% filter(year %in% names(dlist_q4)) %>% droplevels()

# Create data holding object
sal_data_list_q1 <- list()
sal_data_list_q4 <- list()

# Create factor year for indexing the list in the loop
d_sub_sal_q1$year_f <- as.factor(d_sub_sal_q1$year)
d_sub_sal_q4$year_f <- as.factor(d_sub_sal_q4$year)

# Loop through each year and extract raster values for the cpue data points
for(i in unique(d_sub_sal_q1$year_f)) {
  
  # Set plot limits
  ymin = 54; ymax = 58; xmin = 12; xmax = 22
  
  # Subset a year
  sal_slice_q1 <- dlist_q1[[i]]
  sal_slice_q4 <- dlist_q4[[i]]
  
  # Create raster for that year (i)
  r_q1 <- raster(t(sal_slice_q1), xmn = min(lon), xmx = max(lon), ymn = min(lat), ymx = max(lat),
                 crs = CRS("+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs+ towgs84=0,0,0"))
  r_q4 <- raster(t(sal_slice_q4), xmn = min(lon), xmx = max(lon), ymn = min(lat), ymx = max(lat),
                 crs = CRS("+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs+ towgs84=0,0,0"))
  
  # Flip...
  r_q1 <- flip(r_q1, direction = 'y')
  r_q4 <- flip(r_q4, direction = 'y')
  
  plot(r_q1, main = paste(i, "Q1"))
  plot(r_q4, main = paste(i, "Q4"))
  
  # Change projection to UTM (same as pred grid)
  proj_raster_q1 <- projectRaster(r_q1, crs = sr)
  proj_raster_q4 <- projectRaster(r_q4, crs = sr)
  
  # Filter the same year (i) in the cpue data and select only coordinates
  d_slice_q1 <- d_sub_sal_q1 %>% filter(year_f == i) %>% dplyr::select(X, Y)
  d_slice_q4 <- d_sub_sal_q4 %>% filter(year_f == i) %>% dplyr::select(X, Y)
  
  # Make into a SpatialPoints object
  data_sp_q1 <- SpatialPoints(d_slice_q1)
  data_sp_q4 <- SpatialPoints(d_slice_q4)
  
  # Extract raster value (salinity)
  rasValue_q1 <- raster::extract(proj_raster_q1, data_sp_q1)
  rasValue_q4 <- raster::extract(proj_raster_q4, data_sp_q4)
  
  # Now we want to plot the results of the raster extractions by plotting the cpue
  # data points over a raster and saving it for each year.
  # Make the SpatialPoints object into a raster again (for pl)
  df_q1 <- as.data.frame(data_sp_q1)
  df_q4 <- as.data.frame(data_sp_q4)
  
  # Add in the raster value in the df holding the coordinates for the cpue data
  d_slice_q1$sal <- rasValue_q1
  d_slice_q4$sal <- rasValue_q4
  
  # Add in which year
  d_slice_q1$year <- i
  d_slice_q4$year <- i
  
  # Create a index for the data last where we store all years (because our loop index
  # i is not continuous, we can't use it directly)
  index_q1 <- as.numeric(d_slice_q1$year)[1] - 1992
  index_q4 <- as.numeric(d_slice_q4$year)[1] - 1992
  
  # Add each years' data in the list
  sal_data_list_q1[[index_q1]] <- d_slice_q1
  sal_data_list_q4[[index_q4]] <- d_slice_q4
  
}

# Now create a data frame from the list of all annual values
big_dat_sal_q1 <- dplyr::bind_rows(sal_data_list_q1)
big_dat_sal_q4 <- dplyr::bind_rows(sal_data_list_q4)
big_dat_sal <- bind_rows(mutate(big_dat_sal_q1, quarter = 1),
                          mutate(big_dat_sal_q4, quarter = 4))

# Create an ID for matching the salinity data with the cpue data
dat$id_sal <- paste(dat$year, dat$quarter, dat$X, dat$Y, sep = "_")
big_dat_sal$id_sal <- paste(big_dat_sal$year, big_dat_sal$quarter, big_dat_sal$X, big_dat_sal$Y, sep = "_")

# Which id's are not in the pred grid? (It's because I don't have those years, not about the location)
ids <- dat$id_sal[!dat$id_sal %in% c(big_dat_sal$id_sal)]

unique(ids)
length(unique(ids))
length(unique(dat$id_sal))

# Select only the columns we want to merge
big_dat_sub_sal <- big_dat_sal %>% dplyr::select(id_sal, sal)

# Remove duplicate ID (one salinity value per id)
big_dat_sub_sal2 <- big_dat_sub_sal %>% distinct(id_sal, .keep_all = TRUE)
```

```{r merge oxy, temp and salinity data with fish data}
big_dat_sub_oxy2 <- big_dat_sub_oxy2 %>% rename(id_env = id_oxy)
big_dat_sub_temp2 <- big_dat_sub_temp2 %>% rename(id_env = id_temp)
big_dat_sub_sal2 <- big_dat_sub_sal2 %>% rename(id_env = id_sal)

env_dat <- left_join(big_dat_sub_oxy2, big_dat_sub_temp2)

env_dat <- left_join(env_dat,
                     big_dat_sub_sal2)

head(env_dat)
tail(env_dat)

colnames(dat)
tail(dat)
head(dat)

dat <- dat %>%
  dplyr::select(-id_oxy, -id_sal) %>%
  rename(id_env = id_temp) %>%
  left_join(env_dat) %>%
  dplyr::select(-id_env)
  
dat <- dat %>% drop_na(temp) %>% drop_na(oxy) %>% drop_na(sal)

# Temperature
plot_map_fc + 
  geom_raster(data = dat, aes(X, Y, fill = temp)) + 
  facet_wrap(~year)

# Oxygen
plot_map_fc + 
  geom_raster(data = dat, aes(X, Y, fill = oxy)) +
  facet_wrap(~year)

# Salinity
plot_map_fc + 
  geom_raster(data = dat, aes(X, Y, fill = sal)) +
  facet_wrap(~year)

pred_grid <- dat
```

## Add ICES areas

```{r ices areas}
# https://stackoverflow.com/questions/34272309/extract-shapefile-value-to-point-with-r
# https://gis.ices.dk/sf/
shape <- shapefile("data/ICES_StatRec_mapto_ICES_Areas/StatRec_map_Areas_Full_20170124.shp")
head(shape)

pts <- SpatialPoints(cbind(pred_grid$lon, pred_grid$lat), 
                     proj4string = CRS(proj4string(shape)))

pred_grid$subdiv <- over(pts, shape)$Area_27

# Rename subdivisions to the more common names and do some more filtering (by sub div and area)
sort(unique(pred_grid$subdiv))

pred_grid <- pred_grid %>% 
  mutate(sub_div = factor(subdiv),
         sub_div = fct_recode(subdiv,
                              "24" = "3.d.24",
                              "25" = "3.d.25",
                              "26" = "3.d.26",
                              "27" = "3.d.27",
                              "28" = "3.d.28.1",
                              "28" = "3.d.28.2",
                              "29" = "3.d.29"),
         sub_div = as.character(sub_div)) %>% 
  filter(sub_div %in% c("24", "25", "26", "27", "28", 2)) %>% 
  filter(lat > 54 & lat < 59 & lon < 22)

# Add ICES rectangles
pred_grid$ices_rect <- mapplots::ices.rect2(lon = pred_grid$lon, lat = pred_grid$lat)

plot_map +
  geom_raster(data = filter(pred_grid, year == 1999), aes(X, Y)) +
  facet_wrap(~sub_div)

pred_grid <- pred_grid %>% dplyr::select(-subdiv)
```

## Saduria biomass densities

```{r, saduria densities, message=FALSE}
saduria <- raster("data/saduria_tif/FWBiomassm_raster_19812019presHighweightcor_no0_newZi.tif")
saduria_longlat = projectRaster(saduria, crs = ('+proj=longlat'))

plot(saduria_longlat)

# Now extract the values from the saduria raster to the prediction grid
pred_grid$density_saduria <- extract(saduria_longlat, pred_grid %>% dplyr::select(lon, lat))

# Finest scale
plot_map + 
  geom_raster(data = filter(pred_grid, year == 1999), aes(X, Y, fill = density_saduria))
```

## Sprat and herring biomasses

```{r sprat and herring, message=FALSE}
# Read data on rectangle level
spr <- read_xlsx("data/BIAS/N and B per Rect. 1991-2020.xlsx",
                 sheet = 4) %>%
  mutate(sub_div = ifelse(Sub_Div == "28_2", "28", Sub_Div)) %>% 
  filter(sub_div %in% c("24", "25", "26", "27", "28")) %>% 
  rename("ices_rect" = "RECT",
         "Year" = "ANNUS") %>%
  mutate_at(vars(`1`, `2`, `3`, `4`, `5`, `6`, `7`, `8`), ~replace_na(., 0)) %>% # I need to replace NA with 0, else I can't sum! According to Olavi who sent the data, NA means 0 and nothing else. Rectangle*year combinations that do not have information about biomass are simply not included in this data
  mutate(ices_rect = as.factor(ices_rect),
         Species = "Sprat",
         biomass_spr = `1`+`2`+`3`+`4`+`5`+`6`+`7`+`8`, 
         IDr = paste(ices_rect, Year, 4, sep = ".")) # Make new ID, 4 for quarter

her <- read_xlsx("data/BIAS/N and B per Rect. 1991-2020.xlsx",
                 sheet = 3) %>%
  mutate(sub_div = ifelse(Sub_Div == "28_2", "28", Sub_Div)) %>% 
  filter(sub_div %in% c("24", "25", "26", "27", "28")) %>% 
  rename("ices_rect" = "RECT",
         "Year" = "ANNUS") %>%
  mutate_at(vars(`1`, `2`, `3`, `4`, `5`, `6`, `7`, `8`), ~replace_na(., 0)) %>%
  mutate(ices_rect = as.factor(ices_rect),
         Species = "Herring",
         biomass_her = `1`+`2`+`3`+`4`+`5`+`6`+`7`+`8`, 
         IDr = paste(ices_rect, Year, 4, sep = ".")) # Make new ID, 4 for quarter

# Plot distribution over time in the whole area

# How many unique rows per IDr?
her %>%
  group_by(IDr) %>% 
  mutate(n = n()) %>% 
  ggplot(., aes(factor(n))) + geom_bar()

spr %>%
  group_by(IDr) %>% 
  mutate(n = n()) %>% 
  ggplot(., aes(factor(n))) + geom_bar()

# Ok, some ID's with two rows...
spr %>%
  group_by(IDr) %>% 
  mutate(n = n()) %>% 
  filter(n == 2) %>% 
  ungroup() %>% 
  as.data.frame() %>% 
  head(20)

# It's because rectangles somehow being in different sub divisions.
# I need to group by IDr and summarize
spr_sum <- spr %>%
  group_by(IDr) %>% 
  summarise(biomass_spr = sum(biomass_spr)) %>% # Sum abundance within IDr
  distinct(IDr, .keep_all = TRUE) %>% # Remove duplicate IDr
  mutate(ID_temp = IDr) %>% # Create temporary IDr that we can use to split in order
  # to get Year and StatRect back into the summarized data
  separate(ID_temp, c("StatRec", "Year"), sep = 4)

nrow(spr_sum) 
nrow(spr)
nrow(spr %>% group_by(IDr) %>% mutate(n = n()) %>% filter(n == 2))

# Check with a specific rectangle
filter(spr_sum, IDr == "39G4.1991")
filter(spr, IDr == "39G4.1991")

# This should equal 1 (new # rows =  old - duplicated IDr)
nrow(spr_sum) / (nrow(spr) - 0.5*nrow(spr %>% group_by(IDr) %>% mutate(n = n()) %>% filter(n == 2)))

# How many rows per rectangle?
spr_sum %>%
  group_by(IDr) %>% 
  mutate(n = n()) %>% 
  ungroup() %>% 
  distinct(n)

# Now do the same for herring
her_sum <- her %>%
  group_by(IDr) %>% 
  summarise(biomass_her = sum(biomass_her)) %>% # Sum abundance within IDr
  distinct(IDr, .keep_all = TRUE) %>% # Remove duplicate IDr
  mutate(ID_temp = IDr) %>% # Create temporary IDr that we can use to split in order
  # to get Year and StatRect back into the summarized data
  separate(ID_temp, c("StatRec", "Year"), sep = 4)

nrow(her_sum) 
nrow(her)
nrow(her %>% group_by(IDr) %>% mutate(n = n()) %>% filter(n == 2))

filter(her_sum, IDr == "39G4.1991")
filter(her, IDr == "39G4.1991")

# This should equal 1 (new # rows =  old - duplicated IDr)
nrow(her_sum) / (nrow(her) - 0.5*nrow(her %>% group_by(IDr) %>% mutate(n = n()) %>% filter(n == 2)))

# How many rows per rectangle?
her_sum %>%
  group_by(IDr) %>% 
  mutate(n = n()) %>% 
  ungroup() %>% 
  distinct(n)

# Now join pelagic covariates with pred grid
# Make ices_rect a factor in the main data
pred_grid <- pred_grid %>% mutate(ices_rect = as.factor(ices_rect))

# Create IDr in prediction grid to match pelagics data
pred_grid <- pred_grid %>% mutate(IDr = paste(ices_rect, year, quarter, sep = "."))

# Are there any StatRec that are in the prediction grid data that are not in the pelagics data?
# Some very coastal...
unique(pred_grid$ices_rect[!pred_grid$ices_rect %in% her$ices_rect])
unique(pred_grid$ices_rect[!pred_grid$ices_rect %in% spr$ices_rect])

# Check IDr
pred_grid$IDr[!pred_grid$IDr %in% her$IDr]
pred_grid$IDr[!pred_grid$IDr %in% spr$IDr]

# Filter columns so that I only use sprat and herring IDr's that are in the condition data (don't need the others!)
spr_sum <- spr_sum %>% filter(IDr %in% pred_grid$IDr)
her_sum <- her_sum %>% filter(IDr %in% pred_grid$IDr)

# Select columns from pelagic data to go in dat
spr_sub <- spr_sum %>% dplyr::select(IDr, biomass_spr)
her_sub <- her_sum %>% dplyr::select(IDr, biomass_her)

# Now join dat and sprat data
pred_grid <- left_join(pred_grid, spr_sub)

# And herring..
pred_grid <- left_join(pred_grid, her_sub)

# Now deal with the NA's
unique(is.na(spr_sum$biomass_spr))
unique(is.na(her_sum$biomass_her))

unique(is.na(pred_grid$biomass_spr))
unique(is.na(pred_grid$biomass_her))

# The NA's I have in the DAT are missing pelagic data, i.e. not 0's!
# I will keep them as NA here, because else the prediction grid won't be complete in space
# NA's are also for q1...

# Now add in the sub division values
biomass_spr_sd <- read_xlsx("data/BIAS/N and B per SD 1991-2020.xlsx",
                            sheet = 4) %>%
  mutate(sub_div = ifelse(Sub_Div == "28_2", "28", Sub_Div)) %>% 
  filter(sub_div %in% c("24", "25", "26", "27", "28")) %>% 
  rename("Year" = "ANNUS") %>% 
  mutate_at(vars(`AGE1`, `AGE2`, `AGE3`, `AGE4`, `AGE5`, `AGE6`, `AGE7`, `AGE8+`), ~replace_na(., 0)) %>% # I need to replace NA with 0, else I can't sum! According to Olavi who sent the data, NA means 0 and nothing else. Rectangle*year combinations that do not have information about biomass are simply not included in this data
  mutate(sub_div = as.factor(sub_div),
         Species = "Sprat",
         biomass_spr_sd = `AGE1`+`AGE2`+`AGE3`+`AGE4`+`AGE5`+`AGE6`+`AGE7`+`AGE8+`, # omitting `0+` here
         ID_sd_year_quarter = paste(sub_div, Year, 4, sep = ".")) %>% # Make new ID
  dplyr::select(biomass_spr_sd, ID_sd_year_quarter)

biomass_her_sd <- read_xlsx("data/BIAS/N and B per SD 1991-2020.xlsx",
                            sheet = 3) %>%
  mutate(sub_div = ifelse(Sub_Div == "28_2", "28", Sub_Div)) %>% 
  filter(sub_div %in% c("24", "25", "26", "27", "28")) %>% 
  rename("Year" = "ANNUS") %>% 
  mutate_at(vars(`AGE1`, `AGE2`, `AGE3`, `AGE4`, `AGE5`, `AGE6`, `AGE7`, `AGE8+`), ~replace_na(., 0)) %>% # I need to replace NA with 0, else I can't sum! According to Olavi who sent the data, NA means 0 and nothing else. Rectangle*year combinations that do not have information about biomass are simply not included in this data
  mutate(sub_div = as.factor(sub_div),
         Species = "Sprat",
         biomass_her_sd = `AGE1`+`AGE2`+`AGE3`+`AGE4`+`AGE5`+`AGE6`+`AGE7`+`AGE8+`, # omitting `0+` here
         ID_sd_year_quarter = paste(sub_div, Year, 4, sep = ".")) %>% # Make new ID
  dplyr::select(biomass_her_sd, ID_sd_year_quarter)

# Add in the same id to the pred_grid
pred_grid <- pred_grid %>% mutate(ID_sd_year_quarter = paste(sub_div, year, quarter, sep = "."))

pred_grid <- left_join(pred_grid, biomass_spr_sd)

pred_grid <- left_join(pred_grid, biomass_her_sd)

# Plot pelagic biomass densities
plot_map_fc + 
  geom_raster(data = pred_grid, aes(X, Y, fill = biomass_spr)) + 
  facet_wrap(~year)

plot_map_fc + 
  geom_raster(data = pred_grid, aes(X, Y, fill = biomass_her)) + 
  facet_wrap(~year)

plot_map_fc + 
  geom_raster(data = pred_grid, aes(X, Y, fill = biomass_spr_sd)) + 
  facet_wrap(~year)

plot_map_fc + 
  geom_raster(data = pred_grid, aes(X, Y, fill = biomass_her_sd)) + 
  facet_wrap(~year)

# Remove ID
pred_grid <- pred_grid %>% dplyr::select(-ID_sd_year_quarter, -IDr)
```

## Cod and flounder biomass densities

```{r cod and flounder densities}
# # This is so that we can standardize the prediction grid with respect to the data. No need to actually have the correct haul-level catches here
# stomach_dat <- read_csv("data/for_analysis/clean_stomach_data.csv") %>% 
#   rename(id_haul_stomach = id_haul)
# 
# # The models are fitted to scaled depth (standardized wtr to stomach data, so I need to read in that again.)
# stomach_dat_q1_mean_depth <- mean(filter(stomach_dat, quarter == 1)$depth_raster)
# stomach_dat_q1_sd_depth <- sd(filter(stomach_dat, quarter == 1)$depth_raster)
# 
# stomach_dat_q4_mean_depth <- mean(filter(stomach_dat, quarter == 4)$depth_raster)
# stomach_dat_q4_sd_depth <- sd(filter(stomach_dat, quarter == 4)$depth_raster)
# 
# # Scale depth in the pred grid
# pred_grid <- pred_grid %>%
#   mutate(depth_sc = ifelse(quarter == 1,
#                            (depth - stomach_dat_q1_mean_depth) / stomach_dat_q1_sd_depth,
#                            (depth - stomach_dat_q4_mean_depth) / stomach_dat_q4_sd_depth))
# 
# pred_grid_q1 <- pred_grid %>% filter(quarter == 1)
# pred_grid_q4 <- pred_grid %>% filter(quarter == 4)
# 
# # Load models
# mcod_s_q1 <- readRDS("output/mcod_s_q1_v2.rds")
# mcod_s_q4 <- readRDS("output/mcod_s_q4_v2.rds")
# 
# mcod_l_q1 <- readRDS("output/mcod_l_q1_v2.rds")
# mcod_l_q4 <- readRDS("output/mcod_l_q4_v2.rds")
# 
# mfle_q1 <- readRDS("output/mfle_q1_v2.rds")
# mfle_q4 <- readRDS("output/mfle_q4_v2.rds")
# 
# # Predict from the density models
# # Q1
# pred_cod_s_q1 <- predict(mcod_s_q1, newdata = pred_grid_q1)
# pred_cod_l_q1 <- predict(mcod_l_q1, newdata = pred_grid_q1)
# pred_fle_q1 <- predict(mfle_q1, newdata = pred_grid_q1)
# 
# pred_grid_q1$density_cod_small_pred <- exp(pred_cod_s_q1$est)
# pred_grid_q1$density_cod_large_pred <- exp(pred_cod_l_q1$est)
# pred_grid_q1$density_fle_pred <- exp(pred_fle_q1$est)
# 
# # Q4
# pred_cod_s_q4 <- predict(mcod_s_q4, newdata = pred_grid_q4)
# pred_cod_l_q4 <- predict(mcod_l_q4, newdata = pred_grid_q4)
# pred_fle_q4 <- predict(mfle_q4, newdata = pred_grid_q4)
# 
# pred_grid_q4$density_cod_small_pred <- exp(pred_cod_s_q4$est)
# pred_grid_q4$density_cod_large_pred <- exp(pred_cod_l_q4$est)
# pred_grid_q4$density_fle_pred <- exp(pred_fle_q4$est)
# 
# # Combine
# pred_grid <- bind_rows(pred_grid_q1, pred_grid_q4)
```

## Calculate metabolic index

```{r}
# # A_0 is the ratio of constant terms in the Arrhenius equations describing the rates of supply and demand
# # B is individual body size (g)
# # n is the allometric exponent (difference delta - epsilon)
# # E0 is the difference in temperature dependencies of supply and demand
# # T is temperature in degrees Kelvin
# # Tref is an arbitrarily chosen reference temperature (15°C)
# # kb is Boltzmann’s constant (eV K−1).
# 
# # Parameters from Deutsch et al 2015 Science Supp Fig. S2
# A_conc = 9.7*10^-15
# E_conc <- 0.72
# n <- -0.21
# Tref <- 273.15 + 10
# kb <- 0.000086173324
# 
# pred_grid$T_K <- pred_grid$temp + 273.15
# pred_grid$oxy2 <- (pred_grid$oxy * 10^3) / 22.391 # calculate to SI unit mole per cubic meter from ml/L
# pred_grid$phi <- A_conc*(2000^n)*(pred_grid$oxy2 / exp(-E_conc / (kb * pred_grid$T_K)))
```

## Save

```{r save}
# Remove variables and save
pred_grid_93_06 <- pred_grid %>% filter(year < 2007)
pred_grid_07_19 <- pred_grid %>% filter(year > 2006)

write.csv(pred_grid_93_06, file = "data/clean/pred_grid_(1_2).csv", row.names = FALSE)
write.csv(pred_grid_07_19, file = "data/clean/pred_grid_(2_2).csv", row.names = FALSE)
```
